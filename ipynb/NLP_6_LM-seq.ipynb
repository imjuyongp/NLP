{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0e4d15-4091-426f-827d-0843d391f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d37b5-af48-4c14-a6c4-34195c612484",
   "metadata": {},
   "source": [
    "(1) dataset(영문 -> 한글 번역 쌍)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0125f09-9cb2-4dc1-92a6-ef658dd20d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 <sos> (start of sentence), <eos> (end of sentence) 토큰을 추가하여\n",
    "# 각 문장 시작과 끝에 모델이 명시적으로 학습할 수 있도록 한다.\n",
    "data_pairs = [\n",
    "    (\"<sos> i love you <eos>\",\"<sos> 나는 너를 사랑해 <eos>\"),\n",
    "    (\"<sos> he is a student <eos>\", \"<sos> 그는 학생이다 <eos>\"),\n",
    "    (\"<sos> we are friends <eos>\", \"<sos> 우리는 친구다 <eos>\"),\n",
    "    (\"<sos> she is kind <eos>\",\"<sos> 그녀는 친절하다 <eos>\"),\n",
    "    (\"<sos> he is a awesome <eos>\", \"<sos> 그는 멋지다 <eos>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f7129-c13f-4ac8-b77c-57a8dfa4cf4f",
   "metadata": {},
   "source": [
    "(2) 단어 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2ddc24-5f81-44a1-8bac-e65182aba3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전(vocabulary) 생성 함수\n",
    "# 주어진 문장 집합을 입력받아 단어별로 고유한 인덱스를 부여하는 함수.\n",
    "def build_vocab(sentences):\n",
    "    vocab = {\"<pad>\": 0} # 패딩 토큰을 0번 인덱스로 지정\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b832956-ab69-425a-aa43-ad91a3f44055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문(source)과 한글(target) 각각의 단어 사전 생성\n",
    "src_vocab = build_vocab([p[0] for p in data_pairs])\n",
    "trg_vocab = build_vocab([p[1] for p in data_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8aaad62-4fb9-4df7-9003-b4fd674f0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 개수 확인\n",
    "src_vocab_size = len(src_vocab)\n",
    "trg_vocab_size = len(trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae8f335-41de-4c3a-8cde-b1d4758e12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC vocab: {'<pad>': 0, '<sos>': 1, 'i': 2, 'love': 3, 'you': 4, '<eos>': 5, 'he': 6, 'is': 7, 'a': 8, 'student': 9, 'we': 10, 'are': 11, 'friends': 12, 'she': 13, 'kind': 14, 'awesome': 15}\n",
      "TRG vocab: {'<pad>': 0, '<sos>': 1, '나는': 2, '너를': 3, '사랑해': 4, '<eos>': 5, '그는': 6, '학생이다': 7, '우리는': 8, '친구다': 9, '그녀는': 10, '친절하다': 11, '멋지다': 12}\n"
     ]
    }
   ],
   "source": [
    "# 작업 확인용 출력\n",
    "print(\"SRC vocab:\", src_vocab)\n",
    "print(\"TRG vocab:\", trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa68b7-4e94-4df1-af73-ceb3095357b2",
   "metadata": {},
   "source": [
    "(4) 인코더(Encoder) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3860de28-c1ae-445e-9a0b-2cfeb5099c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 함수 정의\n",
    "# 입력 문장을 벡터로 임베딩 후 LSTM을 통해 문맥 벡터(hidden, cell) 생성\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) # 단어 -> 임베딩 벡터\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim) # LSTM으로 문맥 인코딩\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa424d4-5514-49b8-a07b-9ed284a97a7f",
   "metadata": {},
   "source": [
    "(5) 디코더(Decoder) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6c1eaab-1689-42bf-9b28-0526b8850c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 함수 정의\n",
    "# 인코더에서 전달받은 문맥 벡터를 기반으로 한 단어씩 번역을 생성\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden,cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189237b-d4f1-4bd1-9db0-9b006961150f",
   "metadata": {},
   "source": [
    "(6) 인코더-디코더 결합한 Seq2Seq 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6878c5c1-7b1d-4308-afc1-d57b001746d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더-디코더 결합 모델\n",
    "# Encoder의 출력을 디코더에 전달하여 문장 전체를 번역\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.1):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "\n",
    "        # 타임스텝별로 단어 예측 수행\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38a640-86b8-4ab1-b9c1-ece4724a2eb7",
   "metadata": {},
   "source": [
    "(7) 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3c5b8ba-53b8-4f94-b6bf-a7a758cfa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = src_vocab_size # 입력 단어 수\n",
    "OUTPUT_DIM = trg_vocab_size # 출력 단어 수\n",
    "EMB_DIM = 32 # 임베딩 벡터 차원\n",
    "HID_DIM = 64 # LSTM 은닉 상태 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7a9ca-461d-48be-8d66-36148b4bcfb1",
   "metadata": {},
   "source": [
    "- 모델 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c08be65-cab9-4a51-81f8-24cf74035d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n",
    "model = Seq2Seq(enc, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc7d33-99ab-48b1-9dba-8ab9b4e395ab",
   "metadata": {},
   "source": [
    "(8) 학습 준비 (손실함수 및 옵티마이저)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01c21bea-9ccc-452c-b015-2245935739a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb530bb-f5cc-40e3-9477-8d2ebf2dd919",
   "metadata": {},
   "source": [
    "(9) 학습 데이터 텐서 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "868bae6d-52a9-4e91-9e8e-fe8b29b8c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 숫자 텐서로 변환하는 함수\n",
    "# 각 단어를 사전에 따라 인덱스로 바꾸고, (길이, 배치) 형태로 reshape.\n",
    "def tensorize(sentence, vocab):\n",
    "    idxs = [vocab[w] for w in sentence.split()]     # 각 단어를 숫자로 변환\n",
    "    return torch.tensor(idxs, dtype=torch.long).unsqueeze(1)  # (seq_len, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e5b8df0-60dc-419d-818d-28342dfa5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensors = [tensorize(src, src_vocab) for src, _ in data_pairs]\n",
    "trg_tensors = [tensorize(trg, trg_vocab) for _, trg in data_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cba49b-5af7-485f-86e0-b608cace433f",
   "metadata": {},
   "source": [
    "(10) 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24b44fe3-6149-45ae-b31c-9b5c1c97aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 12.328947\n",
      "Epoch 50 | Loss: 0.011107\n",
      "Epoch 100 | Loss: 0.003504\n",
      "Epoch 150 | Loss: 0.001790\n",
      "Epoch 200 | Loss: 0.001103\n",
      "Epoch 250 | Loss: 0.000749\n"
     ]
    }
   ],
   "source": [
    "# 각 에폭마다 모든 문장을 학습하고 손실 값 출력\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    for src_tensor, trg_tensor in zip(src_tensors, trg_tensors):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_tensor, trg_tensor)\n",
    "        output_dim = output.shape[-1]\n",
    "        # <sos> 제외 후 손실 계산\n",
    "        loss = criterion(output[1:].view(-1, output_dim), trg_tensor[1:].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {total_loss:3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d08f05-787e-4953-aa41-605eab4098df",
   "metadata": {},
   "source": [
    "(11) 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50359e67-7bdc-46fc-bdb5-b0f1ad6707bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 함수 정의\n",
    "# 학습된 모델을 이용해 새로운 문장을 번역\n",
    "def translate(sentence):\n",
    "    model.eval() \n",
    "\n",
    "    sentence = \"<sos> \" + sentence + \" <eos>\"\n",
    "\n",
    "    src_idx = torch.tensor([[src_vocab[w] for w in sentence.split()]], dtype=torch.long).T\n",
    "\n",
    "    hidden, cell = model.encoder(src_idx)\n",
    "\n",
    "    input = torch.tensor([trg_vocab[\"<sos>\"]])\n",
    "    result = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "        top1 = output.argmax(1)\n",
    "        word = [k for k, v in trg_vocab.items() if v == top1.item()][0]\n",
    "        if word == \"<eos>\":\n",
    "            break\n",
    "        result.append(word)\n",
    "        input = top1\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e68b70-3ec0-4ee9-b6b7-c73649978ae7",
   "metadata": {},
   "source": [
    "### ---- 번역기 테스트 ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46ba3e25-c4fe-4629-9ec0-d805b20bd9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 너를 사랑해\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"i love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfd1580c-e0c6-42a7-b9a0-5ac11169fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리는 친구다\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"we are friends\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b40c77ed-f81c-4194-adb7-5117ec9a7dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'am'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi am awesome\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      6\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m src_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[src_vocab[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39msplit()]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     10\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(src_idx)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([trg_vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      6\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m src_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43msrc_vocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39msplit()]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     10\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(src_idx)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([trg_vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'am'"
     ]
    }
   ],
   "source": [
    "print(translate(\"i am awesome\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c5ba4-22d4-404d-a4f7-45953eeca35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP\n(NLP_Lec_env)",
   "language": "python",
   "name": "nlp_lec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
