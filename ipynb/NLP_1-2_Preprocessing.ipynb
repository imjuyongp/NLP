{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5668a4-8c62-4bbd-9bea-b40d3d6c372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('/Users/parkjuyong/nltk_data')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef7cbcea-91bc-427b-9ca4-7dc153ccf37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/parkjuyong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/parkjuyong/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/parkjuyong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/parkjuyong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/parkjuyong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 리소스 다운로드 (최초 1회)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51355dc6-7c8d-4c08-8bbb-411eee78ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews =[\n",
    "    \"I loved the movie! 😄 The plot was amazing, but the ending was horrible...\",\n",
    "    \"Terrible movie. Waste of time! <br> 0/10\",\n",
    "    \"It was okay; not great, not bad. Could have been better!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd191b69-472f-4d5a-bc4d-ff62428c33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS 기반 Lemmatization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): # 형용사\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e179a3a-ba44-49b2-b6b1-c19f37ae70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #(1)노이즈 제거\n",
    "    #HTML 태그 제거 : [정규식].*? => 0개 이상의 임의 문자\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #이모지, 특수문자 제거 : [정규식] \\w : 단어문자 (영문자 a-z, A-Z, 숫자 0-9, 밑줄 _)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    #숫자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #공백 정리\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #(2)토큰화\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    #(3)불용어 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens2 = [t for t in tokens if t.lower() not in stop_words]\n",
    "\n",
    "    #(4)어간 추출\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    #(5)표제어 추출\n",
    "    pos_tags = pos_tag(tokens) #표제어 추출에 사용할 품사 태깅\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_tokens_POS = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokens': tokens2,\n",
    "        'stemmed': stemmed_tokens,\n",
    "        'lemmatized': lemmatized_tokens,\n",
    "        'lemmatized_POS': lemmatized_tokens_POS\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8fc219-f779-4cec-8afd-2958fc8331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 실행\n",
    "preprocessed_reviews = [preprocess_text(r) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9296cf61-7fff-4bb4-9c75-30acbe13a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Review 1 ---\n",
      "Originla: I loved the movie The plot was amazing but the ending was horrible\n",
      "Stopword removed Tokens: ['loved', 'movie', 'plot', 'amazing', 'ending', 'horrible']\n",
      "Stemmed: ['i', 'love', 'the', 'movi', 'the', 'plot', 'wa', 'amaz', 'but', 'the', 'end', 'wa', 'horribl']\n",
      "Lemmatized: ['I', 'loved', 'the', 'movie', 'The', 'plot', 'wa', 'amazing', 'but', 'the', 'ending', 'wa', 'horrible']\n",
      "Lemmatized_POS: ['I', 'love', 'the', 'movie', 'The', 'plot', 'be', 'amaze', 'but', 'the', 'end', 'be', 'horrible']\n",
      "\n",
      "---Review 2 ---\n",
      "Originla: Terrible movie Waste of time\n",
      "Stopword removed Tokens: ['Terrible', 'movie', 'Waste', 'time']\n",
      "Stemmed: ['terribl', 'movi', 'wast', 'of', 'time']\n",
      "Lemmatized: ['Terrible', 'movie', 'Waste', 'of', 'time']\n",
      "Lemmatized_POS: ['Terrible', 'movie', 'Waste', 'of', 'time']\n",
      "\n",
      "---Review 3 ---\n",
      "Originla: It was okay not great not bad Could have been better\n",
      "Stopword removed Tokens: ['okay', 'great', 'bad', 'Could', 'better']\n",
      "Stemmed: ['it', 'wa', 'okay', 'not', 'great', 'not', 'bad', 'could', 'have', 'been', 'better']\n",
      "Lemmatized: ['It', 'wa', 'okay', 'not', 'great', 'not', 'bad', 'Could', 'have', 'been', 'better']\n",
      "Lemmatized_POS: ['It', 'be', 'okay', 'not', 'great', 'not', 'bad', 'Could', 'have', 'be', 'well']\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "for i, result in enumerate(preprocessed_reviews):\n",
    "    print(f\"\\n---Review {i+1} ---\")\n",
    "    print(\"Originla:\", result['original'])\n",
    "    print(\"Stopword removed Tokens:\", result['tokens'])\n",
    "    print(\"Stemmed:\", result['stemmed'])\n",
    "    print(\"Lemmatized:\", result['lemmatized'])\n",
    "    print(\"Lemmatized_POS:\", result['lemmatized_POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0384106-b3f7-4c7d-bf5b-38c1f84f35a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP\n(NLP_Lec_env)",
   "language": "python",
   "name": "nlp_lec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
