{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1249b89c-8823-4326-abfb-85bc3ab3ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a57333-1ab4-4d77-a3c7-fff91dd1e95b",
   "metadata": {},
   "source": [
    "### 1) 단어 사전 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac60b59e-12ba-4415-ac12-061a1e6172a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"cat\", \"dog\", \"book\", \"run\", \"eat\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i:word for word, i in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b023218-4264-406f-9681-a0330b20ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전 : {'cat': 0, 'dog': 1, 'book': 2, 'run': 3, 'eat': 4}\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 사전 :\", word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b9b72-2614-49f6-b3f7-545d7ffc402b",
   "metadata": {},
   "source": [
    "### 2) One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa987f3-a311-4394-8826-2f2625db6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word, vocab_size=len(vocab)):\n",
    "    vec = torch.zeros(vocab_size)\n",
    "    vec[word_to_idx[word]]=1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8055bc-3571-4942-977f-b4ae8bbae226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원-핫 벡터 (dog) : tensor([0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n원-핫 벡터 (dog) :\", one_hot_encode(\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103ff33-30d5-4a4f-b585-24b284f56d5e",
   "metadata": {},
   "source": [
    "### 3) Embedding Matrix : (5 x 3 크기의 행렬, 단어 5개 x 차원 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb68236f-3c95-47c6-9a7b-6727dad5c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3 # 차원 수 (d)\n",
    "embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b9e06a-1b3c-4ff5-8000-1e2027c3cf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "임베딩 행렬 (학습 전 초기 값):\n",
      "Parameter containing:\n",
      "tensor([[ 1.9409,  0.9565, -0.0133],\n",
      "        [-0.1943,  0.8660,  0.4790],\n",
      "        [ 0.1283,  0.0890,  0.5064],\n",
      "        [ 0.9608,  1.9671, -1.4872],\n",
      "        [-0.8297,  1.7280,  1.2303]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n임베딩 행렬 (학습 전 초기 값):\")\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166e156-2bd7-49ba-abf8-5df03829ef84",
   "metadata": {},
   "source": [
    "### 4) Embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d0611d-0323-45d4-b110-103e24937181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터를 추출할 단어 설정\n",
    "word = \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287c693a-cc99-4c35-b3b9-45b9cfedd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = torch.tensor([word_to_idx[word]])\n",
    "embed_vec = embedding(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a88d3d-1faf-45b2-8981-f8647a95476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 'dog'의 임베딩 벡터:\n",
      "tensor([[-0.1943,  0.8660,  0.4790]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n단어 '{word}'의 임베딩 벡터:\")\n",
    "print(embed_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81df0c-5b61-489e-a2a7-3f9fe2a82194",
   "metadata": {},
   "source": [
    "## 은닉층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3641d920-c3ab-4f74-b739-4e5b7a4260bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b105818b-7f85-4674-b3f6-ec578246b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 차원 (예 : 단어 임베딩 4차원), 은닉층 3차원\n",
    "input_dim = 4\n",
    "hidden_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b50ac6-207a-4d73-ac74-50714875dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 입력 (배치 = 2, 임베딩 차원 = 4)\n",
    "x = torch.tensor([[0.1,0.2,0.3,0.4],\n",
    "                  [0.5,0.6,0.7,0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47e4c44-a5b5-4a40-8249-5e3e18fca4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 변환 (wx + b)\n",
    "linear = nn.Linear(input_dim, hidden_dim)\n",
    "h_linear = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc79365-63b6-49b3-a408-b323e0c35770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선형 변환 결과 :  tensor([[0.2165, 0.3851, 0.2727],\n",
      "        [0.1281, 0.6987, 0.1606]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"선형 변환 결과 : \", h_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194bfdd0-e223-489b-bee7-6d53ddcb23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활설화 함수 적용\n",
    "h_sigmoid = torch.sigmoid(h_linear)\n",
    "h_tanh = torch.tanh(h_linear)\n",
    "h_relu = F.relu(h_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a3207-5ee8-4de4-86d8-1b2b39c1f333",
   "metadata": {},
   "source": [
    "### ReLU 함수 : f(x)=max(0,x)\n",
    "- 입력이 0보다 크면 그대로 출력, 입력이 0이하면 0 출력\n",
    "- 샘플 입력에 대한 nn.Linear()연산 결과가 모두 0이하 이므로 ReLu 결과 값이 모두 0이 되엇음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "612d1beb-b926-4f0f-ba66-4133828ea241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sigmoid: tensor([[0.5539, 0.5951, 0.5678],\n",
      "        [0.5320, 0.6679, 0.5401]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      " Tanh: tensor([[0.2132, 0.3671, 0.2662],\n",
      "        [0.1274, 0.6035, 0.1593]], grad_fn=<TanhBackward0>)\n",
      "\n",
      " Relu: tensor([[0.2165, 0.3851, 0.2727],\n",
      "        [0.1281, 0.6987, 0.1606]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sigmoid:\", h_sigmoid)\n",
    "print(\"\\n Tanh:\", h_tanh)\n",
    "print(\"\\n Relu:\", h_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8021a-5f18-4d83-8817-68715b1a0099",
   "metadata": {},
   "source": [
    "- 학습 수행 후, 은닉층 활성화 함수 출력 확인\n",
    "  - x : 임의로 가정한 임베딩 값\n",
    "  - y : 임의로 가정한 각 샘플의 정답 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba3aaee3-be18-4447-9976-29c8efd19866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b85de57-e128-4ee0-b64e-2673caee7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0,2.0,3.0,4.0],\n",
    "                  [4.0,3.0,2.0,1.0]])\n",
    "y = torch.tensor([0,2]) # 학습을 위한 정답 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40460fcf-0628-4af6-95fa-7a9f3822eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 : 입력층(4) -> 은닉층(5) -> 출력층(3)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4,5)\n",
    "        self.fc2 = nn.Linear(5,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = F.relu(self.fc1(x)) # ReLU 활성화\n",
    "        output = self.fc2(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1ac79ae-ab24-4539-ab87-52cabc838cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d9174-8762-466e-b828-2824bac1683a",
   "metadata": {},
   "source": [
    "- fc2는 출력층의 선형 변환이고, nn.CrossEntropyLoss()에서 Softmax 함수가 암묵적으로 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14880567-6936-4ed4-9480-4d6d30e8e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31f9c375-d6ae-4a5f-a1e9-579f8e1e4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 수행\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d0ea068-3b10-483d-b542-b857532680fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후  출력\n",
    "output, hidden = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2deaa3a8-5421-468f-99db-c38781fab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉층 출력(ReLU 적용 후):\n",
      " tensor([[0.0000, 0.0000, 5.2137, 0.0000, 0.0000],\n",
      "        [0.0000, 6.8489, 1.8178, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"은닉층 출력(ReLU 적용 후):\\n\", hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871df19-4f5d-4773-920c-2b51ae8e704d",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "595f06d6-7540-448f-82df-96a4ee8b4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['I','love','NLP','.'],\n",
    "    ['You','hate','AI',','],\n",
    "    ['We','study','AI','.'],\n",
    "    ['I','enjoy','learning','.']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c89254-1e2d-4190-bf48-e54ada886cc6",
   "metadata": {},
   "source": [
    "단어 사전 / 단어 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bfa34d2-1fcb-4ebe-a349-e224badb4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(word for sent in sentences for word in sent))\n",
    "word2idx = {w:i for i, w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbe2b69c-7be5-4119-8012-9ce6352bba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study', 'enjoy', 'NLP', 'learning', '.', 'You', 'We', 'I', 'AI', ',', 'hate', 'love']\n"
     ]
    }
   ],
   "source": [
    "print(vocab) # 작업 확인용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e6662fa-136f-4648-a63f-91852f6544b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'study': 0, 'enjoy': 1, 'NLP': 2, 'learning': 3, '.': 4, 'You': 5, 'We': 6, 'I': 7, 'AI': 8, ',': 9, 'hate': 10, 'love': 11}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx) # 작업 확인용 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec9add-8d43-4c65-ad41-6a7ecf53227f",
   "metadata": {},
   "source": [
    "문장 -> 인덱스 시퀀스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "055f8c7a-9b68-4a23-b7aa-fe2aeb47ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence):\n",
    "    return torch.tensor([word2idx[word] for word in sentence], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c03277-2fc8-4d0f-96bf-8e030c1d400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sentence_to_indices(sent)[:-1] for sent in sentences] # 입력\n",
    "Y = [sentence_to_indices(sent)[1:] for sent in sentences] # 다른 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc6d43c-5dde-4630-be22-01617dd306ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 7, 11,  2]), tensor([ 5, 10,  8]), tensor([6, 0, 8]), tensor([7, 1, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e46d521f-35fb-4aa0-bb42-abba9fca083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([11,  2,  4]), tensor([10,  8,  9]), tensor([0, 8, 4]), tensor([1, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "043679cb-f57d-4475-a3c8-c0eea9c42d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 처리용\n",
    "def get_batch(X,Y):\n",
    "    return torch.stack(X), torch.stack(Y) # [batch_size, seq_Len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd8f5c9e-e5e2-488f-bc5b-de2e0fe4bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_Y = get_batch(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9154344-84b6-4c15-9022-9149031e6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7, 11,  2],\n",
      "        [ 5, 10,  8],\n",
      "        [ 6,  0,  8],\n",
      "        [ 7,  1,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2849acd1-8bff-4f58-832b-03145299e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  2,  4],\n",
      "        [10,  8,  9],\n",
      "        [ 0,  8,  4],\n",
      "        [ 1,  3,  4]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f92ec-dff5-4256-bfd8-afbdcfa5d9b7",
   "metadata": {},
   "source": [
    "### 2. 언어 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98513728-4c78-4f60-8c16-d5addbe71776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 AI 기반 언어모델\n",
    "class MinLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MinLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden = nn.Linear(embed_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_Len] -> [batch, seq_Len, embed_size]\n",
    "        x = self.embedding(x)\n",
    "        h = F.relu(self.hidden(x))\n",
    "        out = self.output(h) # Logits\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbccaa5a-492d-48b4-9cf2-664a0c3be7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 8 #16\n",
    "hidden_size = 18 #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8007db4e-435d-4f77-acd5-983ee732246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MinLM(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eff6425-51d0-4777-9cb2-f80c9e616a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/19, Loss: 1.1106\n",
      "Epoch 40/39, Loss: 0.3610\n",
      "Epoch 60/59, Loss: 0.2396\n",
      "Epoch 80/79, Loss: 0.2340\n",
      "Epoch 100/99, Loss: 0.2330\n",
      "Epoch 120/119, Loss: 0.2326\n",
      "Epoch 140/139, Loss: 0.2323\n",
      "Epoch 160/159, Loss: 0.2321\n",
      "Epoch 180/179, Loss: 0.2319\n",
      "Epoch 200/199, Loss: 0.2318\n",
      "Epoch 220/219, Loss: 0.2317\n",
      "Epoch 240/239, Loss: 0.2316\n",
      "Epoch 260/259, Loss: 0.2316\n",
      "Epoch 280/279, Loss: 0.2315\n",
      "Epoch 300/299, Loss: 0.2315\n",
      "Epoch 320/319, Loss: 0.2314\n",
      "Epoch 340/339, Loss: 0.2314\n",
      "Epoch 360/359, Loss: 0.2314\n",
      "Epoch 380/379, Loss: 0.2313\n",
      "Epoch 400/399, Loss: 0.2313\n",
      "Epoch 420/419, Loss: 0.2313\n",
      "Epoch 440/439, Loss: 0.2313\n",
      "Epoch 460/459, Loss: 0.2313\n",
      "Epoch 480/479, Loss: 0.2312\n",
      "Epoch 500/499, Loss: 0.2312\n"
     ]
    }
   ],
   "source": [
    "epochs = 500 # 반복 횟수\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch_X) # [batch, seq_Len, vocab_size]\n",
    "    loss = criterion(output.view(-1, vocab_size), batch_Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424a112-8341-4b72-a535-bc6a7dfa9b85",
   "metadata": {},
   "source": [
    "### 4. 출력1 : 에측 다음 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1ecc23a-4c75-484d-b4b6-530c889bb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 테스트 문장 준비\n",
    "test_sentence = ['I','love','NLP']\n",
    "# 2) 테스트 문장 인덱싱\n",
    "test_idx = torch.tensor([word2idx[w] for w in test_sentence], dtype=torch.long)\n",
    "# 3) 언어모델 입력 -> 다음 단어 확률 출력\n",
    "with torch.no_grad():\n",
    "    logits = model(test_idx)\n",
    "    pred_indices = torch.argmax(logits, dim=1)\n",
    "    pred_words = [idx2word[idx.item()] for idx in pred_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b62ef688-2065-4e02-a136-450724cb3dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: ['I', 'love', 'NLP']\n",
      "예측 다음 단어: ['love', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"입력 문장:\", test_sentence)\n",
    "print(\"예측 다음 단어:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6983660-d273-4813-9a91-bfcc0d4dd04b",
   "metadata": {},
   "source": [
    "### 5. 출력2 : 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7dc474c-5b26-4656-b7ca-3f4200589054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, seed_words, max_len=5):\n",
    "    model.eval()\n",
    "    generated = seed_words[:]\n",
    "    idx_seq = torch.tensor([word2idx[w] for w in seed_words], dtype=torch.long).unsqueeze(0) #[1, seq_len]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_seq) # [1,seq_len, vocab_size]\n",
    "            next_word_logits = logits[0,-1] # 마지막 단어 예측\n",
    "            next_idx = torch.argmax(next_word_logits).item()\n",
    "            next_word = idx2word[next_idx]\n",
    "            generated.append(next_word)\n",
    "\n",
    "            # 다음 입력에 추가\n",
    "            idx_seq = torch.cat([idx_seq, torch.tensor([[next_idx]])], dim=1)\n",
    "\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec6833-7a57-4e7d-bae4-59a9ee061de8",
   "metadata": {},
   "source": [
    "시드 단어로 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b78ebb43-aa99-4bb6-baae-a6a8068d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = ['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94565ed9-24d7-482c-a0cd-1d551d968146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: ['I']\n",
      "Generated: ['I', 'love']\n"
     ]
    }
   ],
   "source": [
    "generated_sentence = generate_sentence(model, seed, max_len=5)\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Generated:\", generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a284b-75c8-4913-968f-a4dc9d6ee533",
   "metadata": {},
   "source": [
    "bi-gram 기반 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d3f8d67-8503-44a7-b977-a3d59c7476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 (Bigram -> Next word)\n",
    "def make_bigram_data(sentences):\n",
    "    inputs, targets = [], []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent) - 2):\n",
    "            print(\"bigram :\", sent[i], sent[i+1])\n",
    "            bigram = [word2idx[sent[i]], word2idx[sent[i+1]]] # 직전 2단어\n",
    "            print(\"target :\", sent[i+2])\n",
    "            target = word2idx[sent[i+2]] # 다음 단어\n",
    "\n",
    "            inputs.append(bigram)\n",
    "            targets.append(target)\n",
    "        print(\"inputs : \", inputs)\n",
    "        print(\"target : \", targets)\n",
    "        return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3bd5b23b-4053-49d0-bcad-44f30db9da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram : I love\n",
      "target : NLP\n",
      "bigram : love NLP\n",
      "target : .\n",
      "inputs :  [[7, 11], [11, 2]]\n",
      "target :  [2, 4]\n"
     ]
    }
   ],
   "source": [
    "X, Y = make_bigram_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9d54f6b-65d2-498b-a5ac-369f26a456f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (bigram):  tensor([[ 7, 11],\n",
      "        [11,  2]])\n",
      "Y (target):  tensor([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"X (bigram): \", X)\n",
    "print(\"Y (target): \", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc042740-4fe4-493e-bacc-57428694c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 \n",
    "class BigramNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(BigramNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * 2, hidden_dim) # 두 단어 벡터 contact\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size) # 출력층(단어 분포)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embeds = self.embedding(x) # (batch, 2, embed_dim)\n",
    "        embeds = embeds.view(x.size(0), -1) # 펼치기 (batch, 2*embed_dim)\n",
    "        hidden = self.relu(self.fc1(embeds))\n",
    "        out = self.fc2(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14580c8a-13cd-4891-ae6a-cfdb4cc3c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "Bimodel = BigramNN(vocab_size, embed_dim, hidden_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Bimodel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "695223df-ccfe-4340-b812-c69696639ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.0001\n",
      "Epoch 100, Loss: 0.0001\n",
      "Epoch 150, Loss: 0.0001\n",
      "Epoch 200, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = Bimodel(X)\n",
    "    loss = criterion(output, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "19040aaf-1303-43af-9bec-29c8a7be1526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bigram Prediction ===\n",
      "Input: (I, love) ->  NLP\n",
      "Input: (You, hate) ->  .\n"
     ]
    }
   ],
   "source": [
    "# 테스트 : 다음 단어 예측\n",
    "def predict_next(w1,w2):\n",
    "    Bimodel.eval()\n",
    "    x = torch.tensor([[word2idx[w1], word2idx[w2]]])\n",
    "    with torch.no_grad():\n",
    "        output = Bimodel(x)\n",
    "        pred_idx = torch.argmax(output, dim=1).item()\n",
    "    return idx2word[pred_idx]\n",
    "\n",
    "print(\"\\n=== Bigram Prediction ===\")\n",
    "print(\"Input: (I, love) -> \" , predict_next(\"I\",\"love\"))\n",
    "print(\"Input: (You, hate) -> \", predict_next(\"You\",\"hate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c949c72-5046-4021-b08f-4ba2276919a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP\n(NLP_Lec_env)",
   "language": "python",
   "name": "nlp_lec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
